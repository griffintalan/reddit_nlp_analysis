{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import VotingClassifier, AdaBoostClassifier, ExtraTreesClassifier, RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.compose import make_column_transformer\n",
    "from nltk.tokenize.regexp import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing formatted data from notebook 1\n",
    "df = pd.read_csv('../Data/combined_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 14)\n",
      "\n",
      "title                  0\n",
      "selftext               0\n",
      "subreddit              0\n",
      "author                 0\n",
      "num_comments           0\n",
      "score                  0\n",
      "timestamp              0\n",
      "days_old               0\n",
      "all_text               0\n",
      "all_text_length        0\n",
      "title_length           0\n",
      "selftext_length        0\n",
      "log_all_text_length    0\n",
      "is_liberal             0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>author</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>days_old</th>\n",
       "      <th>all_text</th>\n",
       "      <th>all_text_length</th>\n",
       "      <th>title_length</th>\n",
       "      <th>selftext_length</th>\n",
       "      <th>log_all_text_length</th>\n",
       "      <th>is_liberal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It has never been more dangerous to live in a ...</td>\n",
       "      <td>Everyone here says the virus is a hoax. The co...</td>\n",
       "      <td>Liberal</td>\n",
       "      <td>readeachbook</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-03-24</td>\n",
       "      <td>29</td>\n",
       "      <td>It has never been more dangerous to live in a ...</td>\n",
       "      <td>502</td>\n",
       "      <td>66</td>\n",
       "      <td>435</td>\n",
       "      <td>6.218600</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The supreme test for Trump supporters</td>\n",
       "      <td>Should they die for Trump by supporting his de...</td>\n",
       "      <td>Liberal</td>\n",
       "      <td>tsdguy</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-03-24</td>\n",
       "      <td>29</td>\n",
       "      <td>The supreme test for Trump supporters Should t...</td>\n",
       "      <td>200</td>\n",
       "      <td>37</td>\n",
       "      <td>162</td>\n",
       "      <td>5.298317</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Conservative on Liberal subreddit.</td>\n",
       "      <td>Ayeeeeee...young conservative on liberal subr...</td>\n",
       "      <td>Liberal</td>\n",
       "      <td>Warhound13</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>2020-03-24</td>\n",
       "      <td>29</td>\n",
       "      <td>Conservative on Liberal subreddit.  Ayeeeeee.....</td>\n",
       "      <td>305</td>\n",
       "      <td>34</td>\n",
       "      <td>270</td>\n",
       "      <td>5.720312</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  It has never been more dangerous to live in a ...   \n",
       "1              The supreme test for Trump supporters   \n",
       "2                 Conservative on Liberal subreddit.   \n",
       "\n",
       "                                            selftext subreddit        author  \\\n",
       "0  Everyone here says the virus is a hoax. The co...   Liberal  readeachbook   \n",
       "1  Should they die for Trump by supporting his de...   Liberal        tsdguy   \n",
       "2   Ayeeeeee...young conservative on liberal subr...   Liberal    Warhound13   \n",
       "\n",
       "   num_comments  score   timestamp  days_old  \\\n",
       "0            14      1  2020-03-24        29   \n",
       "1             9      1  2020-03-24        29   \n",
       "2            52      1  2020-03-24        29   \n",
       "\n",
       "                                            all_text  all_text_length  \\\n",
       "0  It has never been more dangerous to live in a ...              502   \n",
       "1  The supreme test for Trump supporters Should t...              200   \n",
       "2  Conservative on Liberal subreddit.  Ayeeeeee.....              305   \n",
       "\n",
       "   title_length  selftext_length  log_all_text_length  is_liberal  \n",
       "0            66              435             6.218600           1  \n",
       "1            37              162             5.298317           1  \n",
       "2            34              270             5.720312           1  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ensuring import format \n",
    "print(df.shape)\n",
    "print()\n",
    "print(df.isnull().sum())\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating column lemmatizer function\n",
    "\n",
    "def column_lemmatizer(data, column):\n",
    "    tokenizer = RegexpTokenizer(\"[\\w']+\")\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    new_column = []\n",
    "    for i in data[column]:\n",
    "        temp_list = []\n",
    "        tokens = tokenizer.tokenize(i.lower().strip())\n",
    "        for j in tokens:\n",
    "            temp_list.append(lemmatizer.lemmatize(j))\n",
    "        temp_list = ' '.join(temp_list)\n",
    "        new_column.append(temp_list)\n",
    "    return new_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       it ha never been more dangerous to live in a r...\n",
       "1       the supreme test for trump supporter should th...\n",
       "2       conservative on liberal subreddit ayeeeeee you...\n",
       "3       are you willing to steal the highest office if...\n",
       "4       the misreported nature of the new coronavirus ...\n",
       "                              ...                        \n",
       "3995    what if we fought war to win when i wa a young...\n",
       "3996    well it is 2014 only 1004 day left for obama i...\n",
       "3997    how doe r conservative feel about the negative...\n",
       "3998    where i would like to see gop policy shift oka...\n",
       "3999    finding an aca exemption i wa tooling around w...\n",
       "Name: all_text, Length: 4000, dtype: object"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lemmatizing text columns\n",
    "\n",
    "df['all_text'] = column_lemmatizer(df, 'all_text')\n",
    "df['title'] = column_lemmatizer(df, 'title')\n",
    "df['selftext'] = column_lemmatizer(df, 'selftext')\n",
    "df['all_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_alltext = df['all_text']\n",
    "X_title = df['title']\n",
    "X_selftext = df['selftext']\n",
    "X_nums = df[['num_comments', 'score', 'days_old', 'log_all_text_length']]\n",
    "y = df['is_liberal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split each data subset\n",
    "\n",
    "X_train_alltext, X_test_alltext, y_train_alltext, y_test_alltext = train_test_split(X_alltext, y)\n",
    "\n",
    "X_train_title, X_test_title, y_train_title, y_test_title = train_test_split(X_title, y)\n",
    "\n",
    "X_train_selftext, X_test_selftext, y_train_selftext, y_test_selftext = train_test_split(X_selftext, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baseline accuracy score\n",
    "\n",
    "print ('Baseline Accuracy Score: ' + str(round(max(df['is_liberal'].value_counts(normalize=True)), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating model scoring function\n",
    "\n",
    "def gridsearch_score(pipe_model, params, cv, xtrain, ytrain, xtest, ytest):\n",
    "    \n",
    "    grid = GridSearchCV(estimator = pipe_model,\n",
    "                       param_grid = params,\n",
    "                       cv = cv)\n",
    "    \n",
    "    grid.fit(xtrain, ytrain)\n",
    "    \n",
    "    baseline = round(max(df['is_liberal'].value_counts(normalize=True)), 4)\n",
    "    train_score = grid.score(xtrain, ytrain)\n",
    "    test_score = grid.score(xtest, ytest)\n",
    "    best_params = grid.best_params_\n",
    "    \n",
    "    preds = grid.predict(xtest)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(ytest,\n",
    "                                  preds).ravel()\n",
    "    \n",
    "    specificity = round(tn / (tn + fp), 4)\n",
    "    sensitivity = round(tp / (tp + fn), 4)\n",
    "    \n",
    "    print(\"Baseline Accuracy: 0.5\")\n",
    "    print(f\"Train Accuracy: {round(train_score, 4)}\")\n",
    "    print(f\"Test Accuracy: {round(test_score, 4)}\")\n",
    "    print()\n",
    "    print(f\"True Positives: {tp}\")\n",
    "    print(f\"True Negatives: {tn}\")\n",
    "    print(f\"False Positives: {fp}\")\n",
    "    print(f\"False Negatives: {fn}\")\n",
    "    print()\n",
    "    print(f\"Specificity: {specificity}\")\n",
    "    print(f\"Sensitivity: {sensitivity}\")\n",
    "    print()\n",
    "    print(best_params)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating function that scores model on all three text subsets\n",
    "\n",
    "def grid_scores_difftexts(pipe_model, params, cv):\n",
    "    \n",
    "    print('---------------------Title---------------------')\n",
    "    gridsearch_score(pipe_model = pipe_model, \n",
    "                 params = params, \n",
    "                 cv = cv, \n",
    "                 xtrain = X_train_title,\n",
    "                 ytrain = y_train_title,\n",
    "                 xtest = X_test_title,\n",
    "                 ytest = y_test_title)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"\\n\")\n",
    "    print('---------------------Self Text---------------------')\n",
    "    gridsearch_score(pipe_model = pipe_model, \n",
    "                 params = params, \n",
    "                 cv = cv, \n",
    "                 xtrain = X_train_alltext,\n",
    "                 ytrain = y_train_alltext,\n",
    "                 xtest = X_test_alltext,\n",
    "                 ytest = y_test_alltext)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"\\n\")\n",
    "    print('---------------------All Text---------------------')\n",
    "    gridsearch_score(pipe_model = pipe_model, \n",
    "                 params = params, \n",
    "                 cv = cv, \n",
    "                 xtrain = X_train_alltext,\n",
    "                 ytrain = y_train_alltext,\n",
    "                 xtest = X_test_alltext,\n",
    "                 ytest = y_test_alltext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turning off warnings for output formatting\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Countvectorizer/LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# setting parameters, creating pipeline, and scoring model\n",
    "\n",
    "params_cvec_ss_logr = {\n",
    "    'cvec__ngram_range' : [(1, 1), (1, 2)],\n",
    "    'cvec__lowercase' : [ False],\n",
    "    'cvec__stop_words' : ['english'],\n",
    "    'cvec__max_features' : [None, 200],\n",
    "#     'logr__penalty' : ['none', 'l2'],\n",
    "    'logr__C' : [0.0001, 0.001, 0.01],\n",
    "    'ss__with_mean' : [False],\n",
    "    'logr__max_iter' : [100, 500]\n",
    "}\n",
    "\n",
    "pipe_cvec_ss_logr = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('ss', StandardScaler()),\n",
    "    ('logr', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------Title---------------------\n",
      "Baseline Accuracy: 0.5\n",
      "Train Accuracy: 0.9817\n",
      "Test Accuracy: 0.639\n",
      "\n",
      "True Positives: 393\n",
      "True Negatives: 246\n",
      "False Positives: 242\n",
      "False Negatives: 119\n",
      "\n",
      "Specificity: 0.5041\n",
      "Sensitivity: 0.7676\n",
      "\n",
      "{'cvec__lowercase': False, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': 'english', 'logr__C': 0.0001, 'logr__max_iter': 100, 'ss__with_mean': False}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------------Self Text---------------------\n",
      "Baseline Accuracy: 0.5\n",
      "Train Accuracy: 0.983\n",
      "Test Accuracy: 0.628\n",
      "\n",
      "True Positives: 391\n",
      "True Negatives: 237\n",
      "False Positives: 263\n",
      "False Negatives: 109\n",
      "\n",
      "Specificity: 0.474\n",
      "Sensitivity: 0.782\n",
      "\n",
      "{'cvec__lowercase': False, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': 'english', 'logr__C': 0.001, 'logr__max_iter': 100, 'ss__with_mean': False}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------------All Text---------------------\n",
      "Baseline Accuracy: 0.5\n",
      "Train Accuracy: 0.983\n",
      "Test Accuracy: 0.628\n",
      "\n",
      "True Positives: 391\n",
      "True Negatives: 237\n",
      "False Positives: 263\n",
      "False Negatives: 109\n",
      "\n",
      "Specificity: 0.474\n",
      "Sensitivity: 0.782\n",
      "\n",
      "{'cvec__lowercase': False, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': 'english', 'logr__C': 0.001, 'logr__max_iter': 100, 'ss__with_mean': False}\n"
     ]
    }
   ],
   "source": [
    "grid_scores_difftexts(pipe_model = pipe_cvec_ss_logr, params = params_cvec_ss_logr, cv = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfidfVectorizer/LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting parameters, creating pipeline, and scoring model\n",
    "\n",
    "params_tfidf_logr = {\n",
    "    'tfidf__ngram_range' : [(1, 1), (1, 2)],\n",
    "    'tfidf__lowercase' : [ False],\n",
    "    'tfidf__stop_words' : ['english'],\n",
    "    'tfidf__max_features' : [None, 200],\n",
    "#     'logr__penalty' : ['none', 'l2'],\n",
    "    'logr__C' : [0.0001, 0.001, 0.01],\n",
    "    'logr__max_iter' : [100, 500]\n",
    "}\n",
    "\n",
    "pipe_tfidf_logr = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('logr', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------Title---------------------\n",
      "Baseline Accuracy: 0.5\n",
      "Train Accuracy: 0.618\n",
      "Test Accuracy: 0.59\n",
      "\n",
      "True Positives: 139\n",
      "True Negatives: 451\n",
      "False Positives: 37\n",
      "False Negatives: 373\n",
      "\n",
      "Specificity: 0.9242\n",
      "Sensitivity: 0.2715\n",
      "\n",
      "{'logr__C': 0.01, 'logr__max_iter': 100, 'tfidf__lowercase': False, 'tfidf__ngram_range': (1, 1), 'tfidf__stop_words': 'english'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------------Self Text---------------------\n",
      "Baseline Accuracy: 0.5\n",
      "Train Accuracy: 0.8887\n",
      "Test Accuracy: 0.672\n",
      "\n",
      "True Positives: 326\n",
      "True Negatives: 346\n",
      "False Positives: 154\n",
      "False Negatives: 174\n",
      "\n",
      "Specificity: 0.692\n",
      "Sensitivity: 0.652\n",
      "\n",
      "{'logr__C': 0.001, 'logr__max_iter': 100, 'tfidf__lowercase': False, 'tfidf__ngram_range': (1, 2), 'tfidf__stop_words': 'english'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------------All Text---------------------\n",
      "Baseline Accuracy: 0.5\n",
      "Train Accuracy: 0.8887\n",
      "Test Accuracy: 0.672\n",
      "\n",
      "True Positives: 326\n",
      "True Negatives: 346\n",
      "False Positives: 154\n",
      "False Negatives: 174\n",
      "\n",
      "Specificity: 0.692\n",
      "Sensitivity: 0.652\n",
      "\n",
      "{'logr__C': 0.001, 'logr__max_iter': 100, 'tfidf__lowercase': False, 'tfidf__ngram_range': (1, 2), 'tfidf__stop_words': 'english'}\n"
     ]
    }
   ],
   "source": [
    "grid_scores_difftexts(pipe_model = pipe_tfidf_logr, \n",
    "                 params = params_tfidf_logr, \n",
    "                 cv = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer/DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting parameters, creating pipeline, and scoring model\n",
    "\n",
    "params_cvec_dtree = {\n",
    "    'cvec__ngram_range' : [(1, 1), (1, 2)],\n",
    "    'cvec__lowercase' : [ False],\n",
    "    'cvec__stop_words' : ['english'],\n",
    "    'cvec__max_features' : [None, 200],\n",
    "    'dtree__max_features' : [None, 500, 1000],\n",
    "    'dtree__max_depth' : [None, 3, 10]\n",
    "}\n",
    "\n",
    "pipe_cvec_dtree = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('dtree', DecisionTreeClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------Title---------------------\n",
      "Baseline Accuracy: 0.5\n",
      "Train Accuracy: 0.994\n",
      "Test Accuracy: 0.634\n",
      "\n",
      "True Positives: 356\n",
      "True Negatives: 278\n",
      "False Positives: 210\n",
      "False Negatives: 156\n",
      "\n",
      "Specificity: 0.5697\n",
      "Sensitivity: 0.6953\n",
      "\n",
      "{'cvec__lowercase': False, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': 'english', 'dtree__max_depth': None, 'dtree__max_features': None}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------------Self Text---------------------\n",
      "Baseline Accuracy: 0.5\n",
      "Train Accuracy: 0.7317\n",
      "Test Accuracy: 0.617\n",
      "\n",
      "True Positives: 365\n",
      "True Negatives: 252\n",
      "False Positives: 248\n",
      "False Negatives: 135\n",
      "\n",
      "Specificity: 0.504\n",
      "Sensitivity: 0.73\n",
      "\n",
      "{'cvec__lowercase': False, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': 'english', 'dtree__max_depth': 10, 'dtree__max_features': None}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------------All Text---------------------\n",
      "Baseline Accuracy: 0.5\n",
      "Train Accuracy: 0.7323\n",
      "Test Accuracy: 0.614\n",
      "\n",
      "True Positives: 364\n",
      "True Negatives: 250\n",
      "False Positives: 250\n",
      "False Negatives: 136\n",
      "\n",
      "Specificity: 0.5\n",
      "Sensitivity: 0.728\n",
      "\n",
      "{'cvec__lowercase': False, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': 'english', 'dtree__max_depth': 10, 'dtree__max_features': None}\n"
     ]
    }
   ],
   "source": [
    "grid_scores_difftexts(pipe_model = pipe_cvec_dtree, \n",
    "                 params = params_cvec_dtree, \n",
    "                 cv = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfidfVectorizer/DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting parameters, creating pipeline, and scoring model\n",
    "\n",
    "params_tfidf_dtree = {\n",
    "    'tfidf__ngram_range' : [(1, 1), (1, 2)],\n",
    "    'tfidf__lowercase' : [ False],\n",
    "    'tfidf__stop_words' : ['english'],\n",
    "    'tfidf__max_features' : [None, 200],\n",
    "    'dtree__max_features' : [None, 500, 1000],\n",
    "    'dtree__max_depth' : [None, 3, 10]\n",
    "}\n",
    "\n",
    "pipe_tfidf_dtree = Pipeline([\n",
    "    ('tfidf', CountVectorizer()),\n",
    "    ('dtree', DecisionTreeClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------Title---------------------\n",
      "Baseline Accuracy: 0.5\n",
      "Train Accuracy: 0.994\n",
      "Test Accuracy: 0.607\n",
      "\n",
      "True Positives: 346\n",
      "True Negatives: 261\n",
      "False Positives: 227\n",
      "False Negatives: 166\n",
      "\n",
      "Specificity: 0.5348\n",
      "Sensitivity: 0.6758\n",
      "\n",
      "{'dtree__max_depth': None, 'dtree__max_features': 500, 'tfidf__lowercase': False, 'tfidf__ngram_range': (1, 1), 'tfidf__stop_words': 'english'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------------Self Text---------------------\n",
      "Baseline Accuracy: 0.5\n",
      "Train Accuracy: 0.9997\n",
      "Test Accuracy: 0.592\n",
      "\n",
      "True Positives: 309\n",
      "True Negatives: 283\n",
      "False Positives: 217\n",
      "False Negatives: 191\n",
      "\n",
      "Specificity: 0.566\n",
      "Sensitivity: 0.618\n",
      "\n",
      "{'dtree__max_depth': None, 'dtree__max_features': None, 'tfidf__lowercase': False, 'tfidf__ngram_range': (1, 2), 'tfidf__stop_words': 'english'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------------All Text---------------------\n",
      "Baseline Accuracy: 0.5\n",
      "Train Accuracy: 0.7313\n",
      "Test Accuracy: 0.613\n",
      "\n",
      "True Positives: 366\n",
      "True Negatives: 247\n",
      "False Positives: 253\n",
      "False Negatives: 134\n",
      "\n",
      "Specificity: 0.494\n",
      "Sensitivity: 0.732\n",
      "\n",
      "{'dtree__max_depth': 10, 'dtree__max_features': None, 'tfidf__lowercase': False, 'tfidf__ngram_range': (1, 2), 'tfidf__stop_words': 'english'}\n"
     ]
    }
   ],
   "source": [
    "grid_scores_difftexts(pipe_model = pipe_tfidf_dtree, \n",
    "                 params = params_tfidf_dtree, \n",
    "                 cv = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer/KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting parameters, creating pipeline, and scoring model\n",
    "\n",
    "params_cvec_ss_knn = {\n",
    "    'cvec__ngram_range' : [(1, 1), (1, 2)],\n",
    "    'cvec__lowercase' : [ False],\n",
    "    'cvec__stop_words' : ['english'],\n",
    "    'cvec__max_features' : [None, 200],\n",
    "    'ss__with_mean' : [False],\n",
    "    'knn__n_neighbors' : [2, 5, 10],\n",
    "    'knn__metric' : ['manhattan', 'minkowski']\n",
    "}\n",
    "\n",
    "pipe_cvec_ss_knn = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('ss', StandardScaler()),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------Title---------------------\n",
      "Baseline Accuracy: 0.5\n",
      "Train Accuracy: 0.9277\n",
      "Test Accuracy: 0.589\n",
      "\n",
      "True Positives: 206\n",
      "True Negatives: 383\n",
      "False Positives: 105\n",
      "False Negatives: 306\n",
      "\n",
      "Specificity: 0.7848\n",
      "Sensitivity: 0.4023\n",
      "\n",
      "{'cvec__lowercase': False, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': 'english', 'knn__metric': 'manhattan', 'knn__n_neighbors': 2, 'ss__with_mean': False}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------------Self Text---------------------\n",
      "Baseline Accuracy: 0.5\n",
      "Train Accuracy: 0.923\n",
      "Test Accuracy: 0.573\n",
      "\n",
      "True Positives: 406\n",
      "True Negatives: 167\n",
      "False Positives: 333\n",
      "False Negatives: 94\n",
      "\n",
      "Specificity: 0.334\n",
      "Sensitivity: 0.812\n",
      "\n",
      "{'cvec__lowercase': False, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': 'english', 'knn__metric': 'minkowski', 'knn__n_neighbors': 2, 'ss__with_mean': False}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------------All Text---------------------\n",
      "Baseline Accuracy: 0.5\n",
      "Train Accuracy: 0.923\n",
      "Test Accuracy: 0.573\n",
      "\n",
      "True Positives: 406\n",
      "True Negatives: 167\n",
      "False Positives: 333\n",
      "False Negatives: 94\n",
      "\n",
      "Specificity: 0.334\n",
      "Sensitivity: 0.812\n",
      "\n",
      "{'cvec__lowercase': False, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': 'english', 'knn__metric': 'minkowski', 'knn__n_neighbors': 2, 'ss__with_mean': False}\n"
     ]
    }
   ],
   "source": [
    "grid_scores_difftexts(pipe_model = pipe_cvec_ss_knn, \n",
    "                 params = params_cvec_ss_knn, \n",
    "                 cv = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfidfVectorizer/KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting parameters, creating pipeline, and scoring model\n",
    "\n",
    "params_tfidf_knn = {\n",
    "    'tfidf__ngram_range' : [(1, 1), (1, 2)],\n",
    "    'tfidf__lowercase' : [ False],\n",
    "    'tfidf__stop_words' : ['english'],\n",
    "    'tfidf__max_features' : [None, 200],\n",
    "    'knn__n_neighbors' : [2, 5, 10],\n",
    "    'knn__metric' : ['manhattan', 'minkowski']\n",
    "}\n",
    "\n",
    "pipe_tfidf_knn = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('knn', KNeighborsClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------Title---------------------\n",
      "Baseline Accuracy: 0.5\n",
      "Train Accuracy: 0.6723\n",
      "Test Accuracy: 0.569\n",
      "\n",
      "True Positives: 344\n",
      "True Negatives: 225\n",
      "False Positives: 263\n",
      "False Negatives: 168\n",
      "\n",
      "Specificity: 0.4611\n",
      "Sensitivity: 0.6719\n",
      "\n",
      "{'knn__metric': 'manhattan', 'knn__n_neighbors': 5, 'tfidf__lowercase': False, 'tfidf__max_features': 200, 'tfidf__ngram_range': (1, 2), 'tfidf__stop_words': 'english'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------------Self Text---------------------\n",
      "Baseline Accuracy: 0.5\n",
      "Train Accuracy: 0.733\n",
      "Test Accuracy: 0.651\n",
      "\n",
      "True Positives: 322\n",
      "True Negatives: 329\n",
      "False Positives: 171\n",
      "False Negatives: 178\n",
      "\n",
      "Specificity: 0.658\n",
      "Sensitivity: 0.644\n",
      "\n",
      "{'knn__metric': 'minkowski', 'knn__n_neighbors': 10, 'tfidf__lowercase': False, 'tfidf__max_features': None, 'tfidf__ngram_range': (1, 2), 'tfidf__stop_words': 'english'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------------All Text---------------------\n",
      "Baseline Accuracy: 0.5\n",
      "Train Accuracy: 0.733\n",
      "Test Accuracy: 0.651\n",
      "\n",
      "True Positives: 322\n",
      "True Negatives: 329\n",
      "False Positives: 171\n",
      "False Negatives: 178\n",
      "\n",
      "Specificity: 0.658\n",
      "Sensitivity: 0.644\n",
      "\n",
      "{'knn__metric': 'minkowski', 'knn__n_neighbors': 10, 'tfidf__lowercase': False, 'tfidf__max_features': None, 'tfidf__ngram_range': (1, 2), 'tfidf__stop_words': 'english'}\n"
     ]
    }
   ],
   "source": [
    "grid_scores_difftexts(pipe_model = pipe_tfidf_knn, \n",
    "                 params = params_tfidf_knn, \n",
    "                 cv = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer/RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting parameters, creating pipeline, and scoring model\n",
    "\n",
    "params_cvec_rfor = {\n",
    "    'cvec__ngram_range' : [(1, 1), (1, 2)],\n",
    "    'cvec__lowercase' : [ False],\n",
    "    'cvec__stop_words' : ['english']\n",
    "#     'cvec__max_features' : [None, 200],\n",
    "#     'rfor__max_features' : [None, 500, 1000],\n",
    "#     'rfor__max_depth' : [None, 3, 10],\n",
    "#     'rfor__n_estimators' : [50, 100, 200]\n",
    "}\n",
    "\n",
    "pipe_cvec_rfor = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('rfor', RandomForestClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------Title---------------------\n",
      "Baseline Accuracy: 0.5\n",
      "Train Accuracy: 0.994\n",
      "Test Accuracy: 0.647\n",
      "\n",
      "True Positives: 415\n",
      "True Negatives: 232\n",
      "False Positives: 256\n",
      "False Negatives: 97\n",
      "\n",
      "Specificity: 0.4754\n",
      "Sensitivity: 0.8105\n",
      "\n",
      "{'cvec__lowercase': False, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': 'english'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------------Self Text---------------------\n",
      "Baseline Accuracy: 0.5\n",
      "Train Accuracy: 0.9997\n",
      "Test Accuracy: 0.658\n",
      "\n",
      "True Positives: 339\n",
      "True Negatives: 319\n",
      "False Positives: 181\n",
      "False Negatives: 161\n",
      "\n",
      "Specificity: 0.638\n",
      "Sensitivity: 0.678\n",
      "\n",
      "{'cvec__lowercase': False, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': 'english'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------------All Text---------------------\n",
      "Baseline Accuracy: 0.5\n",
      "Train Accuracy: 0.9997\n",
      "Test Accuracy: 0.667\n",
      "\n",
      "True Positives: 351\n",
      "True Negatives: 316\n",
      "False Positives: 184\n",
      "False Negatives: 149\n",
      "\n",
      "Specificity: 0.632\n",
      "Sensitivity: 0.702\n",
      "\n",
      "{'cvec__lowercase': False, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': 'english'}\n"
     ]
    }
   ],
   "source": [
    "grid_scores_difftexts(pipe_model = pipe_cvec_rfor, \n",
    "                 params = params_cvec_rfor, \n",
    "                 cv = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfidfVectorizer/RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting parameters, creating pipeline, and scoring model\n",
    "\n",
    "params_tfidf_rfor = {\n",
    "    'tfidf__ngram_range' : [(1, 1), (1, 2)],\n",
    "    'tfidf__lowercase' : [ False],\n",
    "    'tfidf__stop_words' : ['english'],\n",
    "#     'tfidf__max_features' : [None, 200],\n",
    "#     'rfor__max_features' : [None, 500, 1000],\n",
    "#     'rfor__max_depth' : [None, 3, 10],\n",
    "    'rfor__n_estimators' : [50, 100]\n",
    "}\n",
    "\n",
    "pipe_tfidf_rfor = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('rfor', RandomForestClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------Title---------------------\n",
      "Baseline Accuracy: 0.5\n",
      "Train Accuracy: 0.994\n",
      "Test Accuracy: 0.651\n",
      "\n",
      "True Positives: 381\n",
      "True Negatives: 270\n",
      "False Positives: 218\n",
      "False Negatives: 131\n",
      "\n",
      "Specificity: 0.5533\n",
      "Sensitivity: 0.7441\n",
      "\n",
      "{'rfor__n_estimators': 100, 'tfidf__lowercase': False, 'tfidf__ngram_range': (1, 1), 'tfidf__stop_words': 'english'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------------Self Text---------------------\n",
      "Baseline Accuracy: 0.5\n",
      "Train Accuracy: 0.9997\n",
      "Test Accuracy: 0.67\n",
      "\n",
      "True Positives: 357\n",
      "True Negatives: 313\n",
      "False Positives: 187\n",
      "False Negatives: 143\n",
      "\n",
      "Specificity: 0.626\n",
      "Sensitivity: 0.714\n",
      "\n",
      "{'rfor__n_estimators': 100, 'tfidf__lowercase': False, 'tfidf__ngram_range': (1, 1), 'tfidf__stop_words': 'english'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------------All Text---------------------\n",
      "Baseline Accuracy: 0.5\n",
      "Train Accuracy: 0.9997\n",
      "Test Accuracy: 0.641\n",
      "\n",
      "True Positives: 326\n",
      "True Negatives: 315\n",
      "False Positives: 185\n",
      "False Negatives: 174\n",
      "\n",
      "Specificity: 0.63\n",
      "Sensitivity: 0.652\n",
      "\n",
      "{'rfor__n_estimators': 50, 'tfidf__lowercase': False, 'tfidf__ngram_range': (1, 1), 'tfidf__stop_words': 'english'}\n"
     ]
    }
   ],
   "source": [
    "grid_scores_difftexts(pipe_model = pipe_tfidf_rfor, \n",
    "                 params = params_tfidf_rfor, \n",
    "                 cv = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer/ExtraTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting parameters, creating pipeline, and scoring model\n",
    "\n",
    "params_cvec_xtree = {\n",
    "    'cvec__ngram_range' : [(1, 1), (1, 2)],\n",
    "    'cvec__lowercase' : [ False],\n",
    "    'cvec__stop_words' : ['english']\n",
    "#     'xtree__max_features' : [None, 200],\n",
    "#     'xtree__max_features' : [None, 500, 1000],\n",
    "#     'xtree__max_depth' : [None, 3, 10],\n",
    "#     'xtree__n_estimators' : [50, 100, 200]\n",
    "}\n",
    "\n",
    "pipe_cvec_xtree = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('xtree', ExtraTreesClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------Title---------------------\n",
      "Baseline Accuracy: 0.5\n",
      "Train Accuracy: 0.994\n",
      "Test Accuracy: 0.659\n",
      "\n",
      "True Positives: 380\n",
      "True Negatives: 279\n",
      "False Positives: 209\n",
      "False Negatives: 132\n",
      "\n",
      "Specificity: 0.5717\n",
      "Sensitivity: 0.7422\n",
      "\n",
      "{'cvec__lowercase': False, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': 'english'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------------Self Text---------------------\n",
      "Baseline Accuracy: 0.5\n",
      "Train Accuracy: 0.9997\n",
      "Test Accuracy: 0.645\n",
      "\n",
      "True Positives: 322\n",
      "True Negatives: 323\n",
      "False Positives: 177\n",
      "False Negatives: 178\n",
      "\n",
      "Specificity: 0.646\n",
      "Sensitivity: 0.644\n",
      "\n",
      "{'cvec__lowercase': False, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': 'english'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------------All Text---------------------\n",
      "Baseline Accuracy: 0.5\n",
      "Train Accuracy: 0.9997\n",
      "Test Accuracy: 0.654\n",
      "\n",
      "True Positives: 322\n",
      "True Negatives: 332\n",
      "False Positives: 168\n",
      "False Negatives: 178\n",
      "\n",
      "Specificity: 0.664\n",
      "Sensitivity: 0.644\n",
      "\n",
      "{'cvec__lowercase': False, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': 'english'}\n"
     ]
    }
   ],
   "source": [
    "grid_scores_difftexts(pipe_model = pipe_cvec_xtree, \n",
    "                 params = params_cvec_xtree, \n",
    "                 cv = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfidfVectorizer/ExtraTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting parameters, creating pipeline, and scoring model\n",
    "\n",
    "params_tfidf_xtree = {\n",
    "    'tfidf__ngram_range' : [(1, 1), (1, 2)],\n",
    "    'tfidf__lowercase' : [ False],\n",
    "    'tfidf__stop_words' : ['english']\n",
    "#     'xtree__max_features' : [None, 200],\n",
    "#     'xtree__max_features' : [None, 500, 1000],\n",
    "#     'xtree__max_depth' : [None, 3, 10],\n",
    "#     'xtree__n_estimators' : [50, 100, 200]\n",
    "}\n",
    "\n",
    "pipe_tfidf_xtree = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('xtree', ExtraTreesClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------Title---------------------\n",
      "Baseline Accuracy: 0.5\n",
      "Train Accuracy: 0.994\n",
      "Test Accuracy: 0.661\n",
      "\n",
      "True Positives: 377\n",
      "True Negatives: 284\n",
      "False Positives: 204\n",
      "False Negatives: 135\n",
      "\n",
      "Specificity: 0.582\n",
      "Sensitivity: 0.7363\n",
      "\n",
      "{'tfidf__lowercase': False, 'tfidf__ngram_range': (1, 1), 'tfidf__stop_words': 'english'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------------Self Text---------------------\n",
      "Baseline Accuracy: 0.5\n",
      "Train Accuracy: 0.9997\n",
      "Test Accuracy: 0.666\n",
      "\n",
      "True Positives: 344\n",
      "True Negatives: 322\n",
      "False Positives: 178\n",
      "False Negatives: 156\n",
      "\n",
      "Specificity: 0.644\n",
      "Sensitivity: 0.688\n",
      "\n",
      "{'tfidf__lowercase': False, 'tfidf__ngram_range': (1, 1), 'tfidf__stop_words': 'english'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------------All Text---------------------\n",
      "Baseline Accuracy: 0.5\n",
      "Train Accuracy: 0.9997\n",
      "Test Accuracy: 0.671\n",
      "\n",
      "True Positives: 347\n",
      "True Negatives: 324\n",
      "False Positives: 176\n",
      "False Negatives: 153\n",
      "\n",
      "Specificity: 0.648\n",
      "Sensitivity: 0.694\n",
      "\n",
      "{'tfidf__lowercase': False, 'tfidf__ngram_range': (1, 1), 'tfidf__stop_words': 'english'}\n"
     ]
    }
   ],
   "source": [
    "grid_scores_difftexts(pipe_model = pipe_tfidf_xtree, \n",
    "                 params = params_tfidf_xtree, \n",
    "                 cv = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer/BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting parameters, creating pipeline, and scoring model\n",
    "\n",
    "params_cvec_bag = {\n",
    "    'cvec__ngram_range' : [(1, 1), (1, 2)],\n",
    "    'cvec__lowercase' : [ False],\n",
    "    'cvec__stop_words' : ['english']\n",
    "#     'bag__max_features' : [None, 200],\n",
    "#     'bag__max_features' : [None, 500, 1000],\n",
    "#     'bag__max_depth' : [None, 3, 10],\n",
    "#     'bag__n_estimators' : [50, 100, 200]\n",
    "}\n",
    "\n",
    "pipe_cvec_bag = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('bag', BaggingClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------Title---------------------\n",
      "Baseline Accuracy: 0.5\n",
      "Train Accuracy: 0.975\n",
      "Test Accuracy: 0.631\n",
      "\n",
      "True Positives: 357\n",
      "True Negatives: 274\n",
      "False Positives: 214\n",
      "False Negatives: 155\n",
      "\n",
      "Specificity: 0.5615\n",
      "Sensitivity: 0.6973\n",
      "\n",
      "{'cvec__lowercase': False, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': 'english'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------------Self Text---------------------\n",
      "Baseline Accuracy: 0.5\n",
      "Train Accuracy: 0.9783\n",
      "Test Accuracy: 0.631\n",
      "\n",
      "True Positives: 313\n",
      "True Negatives: 318\n",
      "False Positives: 182\n",
      "False Negatives: 187\n",
      "\n",
      "Specificity: 0.636\n",
      "Sensitivity: 0.626\n",
      "\n",
      "{'cvec__lowercase': False, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': 'english'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------------All Text---------------------\n",
      "Baseline Accuracy: 0.5\n",
      "Train Accuracy: 0.981\n",
      "Test Accuracy: 0.617\n",
      "\n",
      "True Positives: 321\n",
      "True Negatives: 296\n",
      "False Positives: 204\n",
      "False Negatives: 179\n",
      "\n",
      "Specificity: 0.592\n",
      "Sensitivity: 0.642\n",
      "\n",
      "{'cvec__lowercase': False, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': 'english'}\n"
     ]
    }
   ],
   "source": [
    "grid_scores_difftexts(pipe_model = pipe_cvec_bag, \n",
    "                 params = params_cvec_bag, \n",
    "                 cv = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfidfVectorizer/BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting parameters, creating pipeline, and scoring model\n",
    "\n",
    "params_tfidf_bag = {\n",
    "    'tfidf__ngram_range' : [(1, 1), (1, 2)],\n",
    "    'tfidf__lowercase' : [ False],\n",
    "    'tfidf__stop_words' : ['english']\n",
    "#     'bag__max_features' : [None, 200],\n",
    "#     'bag__max_features' : [None, 500, 1000],\n",
    "#     'bag__max_depth' : [None, 3, 10],\n",
    "#     'bag__n_estimators' : [50, 100, 200]\n",
    "}\n",
    "\n",
    "pipe_tfidf_bag = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('bag', BaggingClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------Title---------------------\n",
      "Baseline Accuracy: 0.5\n",
      "Train Accuracy: 0.97\n",
      "Test Accuracy: 0.619\n",
      "\n",
      "True Positives: 347\n",
      "True Negatives: 272\n",
      "False Positives: 216\n",
      "False Negatives: 165\n",
      "\n",
      "Specificity: 0.5574\n",
      "Sensitivity: 0.6777\n",
      "\n",
      "{'tfidf__lowercase': False, 'tfidf__ngram_range': (1, 1), 'tfidf__stop_words': 'english'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------------Self Text---------------------\n",
      "Baseline Accuracy: 0.5\n",
      "Train Accuracy: 0.9847\n",
      "Test Accuracy: 0.623\n",
      "\n",
      "True Positives: 306\n",
      "True Negatives: 317\n",
      "False Positives: 183\n",
      "False Negatives: 194\n",
      "\n",
      "Specificity: 0.634\n",
      "Sensitivity: 0.612\n",
      "\n",
      "{'tfidf__lowercase': False, 'tfidf__ngram_range': (1, 1), 'tfidf__stop_words': 'english'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------------All Text---------------------\n",
      "Baseline Accuracy: 0.5\n",
      "Train Accuracy: 0.982\n",
      "Test Accuracy: 0.609\n",
      "\n",
      "True Positives: 284\n",
      "True Negatives: 325\n",
      "False Positives: 175\n",
      "False Negatives: 216\n",
      "\n",
      "Specificity: 0.65\n",
      "Sensitivity: 0.568\n",
      "\n",
      "{'tfidf__lowercase': False, 'tfidf__ngram_range': (1, 1), 'tfidf__stop_words': 'english'}\n"
     ]
    }
   ],
   "source": [
    "grid_scores_difftexts(pipe_model = pipe_tfidf_bag, \n",
    "                 params = params_tfidf_bag, \n",
    "                 cv = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer/AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting parameters, creating pipeline, and scoring model\n",
    "\n",
    "params_cvec_ada = {\n",
    "    'cvec__ngram_range' : [(1, 1), (1, 2)],\n",
    "    'cvec__lowercase' : [ False],\n",
    "    'cvec__stop_words' : ['english']\n",
    "#     'ada__max_features' : [None, 200],\n",
    "#     'ada__max_features' : [None, 500, 1000],\n",
    "#     'ada__max_depth' : [None, 3, 10],\n",
    "#     'ada__n_estimators' : [100, 1000]\n",
    "}\n",
    "\n",
    "pipe_cvec_ada = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('ada', AdaBoostClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------Title---------------------\n",
      "Baseline Accuracy: 0.5\n",
      "Train Accuracy: 0.6363\n",
      "Test Accuracy: 0.608\n",
      "\n",
      "True Positives: 451\n",
      "True Negatives: 157\n",
      "False Positives: 331\n",
      "False Negatives: 61\n",
      "\n",
      "Specificity: 0.3217\n",
      "Sensitivity: 0.8809\n",
      "\n",
      "{'cvec__lowercase': False, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': 'english'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------------Self Text---------------------\n",
      "Baseline Accuracy: 0.5\n",
      "Train Accuracy: 0.7167\n",
      "Test Accuracy: 0.651\n",
      "\n",
      "True Positives: 365\n",
      "True Negatives: 286\n",
      "False Positives: 214\n",
      "False Negatives: 135\n",
      "\n",
      "Specificity: 0.572\n",
      "Sensitivity: 0.73\n",
      "\n",
      "{'cvec__lowercase': False, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': 'english'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------------All Text---------------------\n",
      "Baseline Accuracy: 0.5\n",
      "Train Accuracy: 0.7167\n",
      "Test Accuracy: 0.651\n",
      "\n",
      "True Positives: 365\n",
      "True Negatives: 286\n",
      "False Positives: 214\n",
      "False Negatives: 135\n",
      "\n",
      "Specificity: 0.572\n",
      "Sensitivity: 0.73\n",
      "\n",
      "{'cvec__lowercase': False, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': 'english'}\n"
     ]
    }
   ],
   "source": [
    "grid_scores_difftexts(pipe_model = pipe_cvec_ada, \n",
    "                 params = params_cvec_ada, \n",
    "                 cv = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfidfVectorizer/AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting parameters, creating pipeline, and scoring model\n",
    "\n",
    "params_tfidf_ada = {\n",
    "    'tfidf__ngram_range' : [(1, 1), (1, 2)],\n",
    "    'tfidf__lowercase' : [ False],\n",
    "    'tfidf__stop_words' : ['english'],\n",
    "#     'ada__max_features' : [None, 200],\n",
    "#     'ada__max_features' : [None, 500, 1000],\n",
    "#     'ada__max_depth' : [None, 3, 10],\n",
    "    'ada__n_estimators' : [100, 200]\n",
    "}\n",
    "\n",
    "pipe_tfidf_ada = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('ada', AdaBoostClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------Title---------------------\n",
      "Baseline Accuracy: 0.5\n",
      "Train Accuracy: 0.6867\n",
      "Test Accuracy: 0.596\n",
      "\n",
      "True Positives: 420\n",
      "True Negatives: 176\n",
      "False Positives: 312\n",
      "False Negatives: 92\n",
      "\n",
      "Specificity: 0.3607\n",
      "Sensitivity: 0.8203\n",
      "\n",
      "{'ada__n_estimators': 100, 'tfidf__lowercase': False, 'tfidf__ngram_range': (1, 1), 'tfidf__stop_words': 'english'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------------Self Text---------------------\n",
      "Baseline Accuracy: 0.5\n",
      "Train Accuracy: 0.7887\n",
      "Test Accuracy: 0.651\n",
      "\n",
      "True Positives: 358\n",
      "True Negatives: 293\n",
      "False Positives: 207\n",
      "False Negatives: 142\n",
      "\n",
      "Specificity: 0.586\n",
      "Sensitivity: 0.716\n",
      "\n",
      "{'ada__n_estimators': 100, 'tfidf__lowercase': False, 'tfidf__ngram_range': (1, 2), 'tfidf__stop_words': 'english'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------------All Text---------------------\n",
      "Baseline Accuracy: 0.5\n",
      "Train Accuracy: 0.7887\n",
      "Test Accuracy: 0.651\n",
      "\n",
      "True Positives: 358\n",
      "True Negatives: 293\n",
      "False Positives: 207\n",
      "False Negatives: 142\n",
      "\n",
      "Specificity: 0.586\n",
      "Sensitivity: 0.716\n",
      "\n",
      "{'ada__n_estimators': 100, 'tfidf__lowercase': False, 'tfidf__ngram_range': (1, 2), 'tfidf__stop_words': 'english'}\n"
     ]
    }
   ],
   "source": [
    "grid_scores_difftexts(pipe_model = pipe_tfidf_ada, \n",
    "                 params = params_tfidf_ada, \n",
    "                 cv = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer/KernelSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting parameters, creating pipeline, and scoring model\n",
    "\n",
    "params_cvec_ss_svm = {\n",
    "    'cvec__ngram_range' : [(1, 1), (1, 2)],\n",
    "    'cvec__lowercase' : [ False],\n",
    "    'cvec__stop_words' : ['english'],\n",
    "    'ss__with_mean' : [False],\n",
    "    'svc__C' : [5, 10, 15]\n",
    "}\n",
    "\n",
    "pipe_cvec_ss_svm = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('ss', StandardScaler()),\n",
    "    ('svc', SVC())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------Title---------------------\n",
      "Baseline Accuracy: 0.5\n",
      "Train Accuracy: 0.9637\n",
      "Test Accuracy: 0.624\n",
      "\n",
      "True Positives: 354\n",
      "True Negatives: 270\n",
      "False Positives: 218\n",
      "False Negatives: 158\n",
      "\n",
      "Specificity: 0.5533\n",
      "Sensitivity: 0.6914\n",
      "\n",
      "{'cvec__lowercase': False, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': 'english', 'ss__with_mean': False, 'svc__C': 5}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------------Self Text---------------------\n",
      "Baseline Accuracy: 0.5\n",
      "Train Accuracy: 0.9847\n",
      "Test Accuracy: 0.591\n",
      "\n",
      "True Positives: 346\n",
      "True Negatives: 245\n",
      "False Positives: 255\n",
      "False Negatives: 154\n",
      "\n",
      "Specificity: 0.49\n",
      "Sensitivity: 0.692\n",
      "\n",
      "{'cvec__lowercase': False, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': 'english', 'ss__with_mean': False, 'svc__C': 10}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------------All Text---------------------\n",
      "Baseline Accuracy: 0.5\n",
      "Train Accuracy: 0.9847\n",
      "Test Accuracy: 0.591\n",
      "\n",
      "True Positives: 346\n",
      "True Negatives: 245\n",
      "False Positives: 255\n",
      "False Negatives: 154\n",
      "\n",
      "Specificity: 0.49\n",
      "Sensitivity: 0.692\n",
      "\n",
      "{'cvec__lowercase': False, 'cvec__ngram_range': (1, 1), 'cvec__stop_words': 'english', 'ss__with_mean': False, 'svc__C': 10}\n"
     ]
    }
   ],
   "source": [
    "grid_scores_difftexts(pipe_model = pipe_cvec_ss_svm, \n",
    "                 params = params_cvec_ss_svm, \n",
    "                 cv = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfidfVectorizer/KernelSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting parameters, creating pipeline, and scoring model\n",
    "\n",
    "params_tfidf_svm = {\n",
    "    'tfidf__ngram_range' : [(1, 1), (1, 2)],\n",
    "    'tfidf__lowercase' : [ False],\n",
    "    'tfidf__stop_words' : ['english'],\n",
    "    'svc__C' : [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "pipe_tfidf_svm = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('svc', SVC())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------Title---------------------\n",
      "Baseline Accuracy: 0.5\n",
      "Train Accuracy: 0.994\n",
      "Test Accuracy: 0.654\n",
      "\n",
      "True Positives: 339\n",
      "True Negatives: 315\n",
      "False Positives: 173\n",
      "False Negatives: 173\n",
      "\n",
      "Specificity: 0.6455\n",
      "Sensitivity: 0.6621\n",
      "\n",
      "{'svc__C': 10, 'tfidf__lowercase': False, 'tfidf__ngram_range': (1, 2), 'tfidf__stop_words': 'english'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------------Self Text---------------------\n",
      "Baseline Accuracy: 0.5\n",
      "Train Accuracy: 0.999\n",
      "Test Accuracy: 0.693\n",
      "\n",
      "True Positives: 342\n",
      "True Negatives: 351\n",
      "False Positives: 149\n",
      "False Negatives: 158\n",
      "\n",
      "Specificity: 0.702\n",
      "Sensitivity: 0.684\n",
      "\n",
      "{'svc__C': 1, 'tfidf__lowercase': False, 'tfidf__ngram_range': (1, 2), 'tfidf__stop_words': 'english'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------------All Text---------------------\n",
      "Baseline Accuracy: 0.5\n",
      "Train Accuracy: 0.999\n",
      "Test Accuracy: 0.693\n",
      "\n",
      "True Positives: 342\n",
      "True Negatives: 351\n",
      "False Positives: 149\n",
      "False Negatives: 158\n",
      "\n",
      "Specificity: 0.702\n",
      "Sensitivity: 0.684\n",
      "\n",
      "{'svc__C': 1, 'tfidf__lowercase': False, 'tfidf__ngram_range': (1, 2), 'tfidf__stop_words': 'english'}\n"
     ]
    }
   ],
   "source": [
    "grid_scores_difftexts(pipe_model = pipe_tfidf_svm, \n",
    "                 params = params_tfidf_svm, \n",
    "                 cv = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom SVM\n",
    "\n",
    "## Combined Text and Numeric Scaled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiatind tfidf and standard scaler\n",
    "\n",
    "tfidf = TfidfVectorizer(lowercase = False, \n",
    "                        ngram_range= (1,2), \n",
    "                        stop_words='english',\n",
    "                        max_features = 20_000\n",
    "                       )\n",
    "\n",
    "ss = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming text with tfidf and scaling numeric data separately\n",
    "\n",
    "X_alltext_tfidf = pd.DataFrame(tfidf.fit_transform(X_alltext).todense())\n",
    "X_nums_ss = pd.DataFrame(ss.fit_transform(X_nums))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recombining features\n",
    "\n",
    "X_combined = pd.concat([X_alltext_tfidf, X_nums_ss], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>19994</th>\n",
       "      <th>19995</th>\n",
       "      <th>19996</th>\n",
       "      <th>19997</th>\n",
       "      <th>19998</th>\n",
       "      <th>19999</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.023769</td>\n",
       "      <td>-0.166980</td>\n",
       "      <td>-1.394170</td>\n",
       "      <td>0.014839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.108492</td>\n",
       "      <td>-0.166980</td>\n",
       "      <td>-1.394170</td>\n",
       "      <td>-0.797496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.620128</td>\n",
       "      <td>-0.166980</td>\n",
       "      <td>-1.394170</td>\n",
       "      <td>-0.425001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.260994</td>\n",
       "      <td>-0.166980</td>\n",
       "      <td>-1.395813</td>\n",
       "      <td>0.407228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.036584</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.416792</td>\n",
       "      <td>-0.166980</td>\n",
       "      <td>-1.395813</td>\n",
       "      <td>2.607957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.193216</td>\n",
       "      <td>0.014234</td>\n",
       "      <td>2.343613</td>\n",
       "      <td>1.693465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.260994</td>\n",
       "      <td>-0.179061</td>\n",
       "      <td>2.341970</td>\n",
       "      <td>-1.028203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.145678</td>\n",
       "      <td>-0.034089</td>\n",
       "      <td>2.341970</td>\n",
       "      <td>0.382072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.162015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.260994</td>\n",
       "      <td>-0.166980</td>\n",
       "      <td>2.341970</td>\n",
       "      <td>0.949919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.193216</td>\n",
       "      <td>0.014234</td>\n",
       "      <td>2.341970</td>\n",
       "      <td>1.361815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows  20004 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0      1      2      3      4      5         6      7      8      9      \\\n",
       "0       0.0    0.0    0.0    0.0    0.0    0.0  0.000000    0.0    0.0    0.0   \n",
       "1       0.0    0.0    0.0    0.0    0.0    0.0  0.000000    0.0    0.0    0.0   \n",
       "2       0.0    0.0    0.0    0.0    0.0    0.0  0.000000    0.0    0.0    0.0   \n",
       "3       0.0    0.0    0.0    0.0    0.0    0.0  0.000000    0.0    0.0    0.0   \n",
       "4       0.0    0.0    0.0    0.0    0.0    0.0  0.036584    0.0    0.0    0.0   \n",
       "...     ...    ...    ...    ...    ...    ...       ...    ...    ...    ...   \n",
       "3995    0.0    0.0    0.0    0.0    0.0    0.0  0.000000    0.0    0.0    0.0   \n",
       "3996    0.0    0.0    0.0    0.0    0.0    0.0  0.000000    0.0    0.0    0.0   \n",
       "3997    0.0    0.0    0.0    0.0    0.0    0.0  0.000000    0.0    0.0    0.0   \n",
       "3998    0.0    0.0    0.0    0.0    0.0    0.0  0.162015    0.0    0.0    0.0   \n",
       "3999    0.0    0.0    0.0    0.0    0.0    0.0  0.000000    0.0    0.0    0.0   \n",
       "\n",
       "      ...  19994  19995  19996  19997  19998  19999     0         1      \\\n",
       "0     ...    0.0    0.0    0.0    0.0    0.0    0.0 -0.023769 -0.166980   \n",
       "1     ...    0.0    0.0    0.0    0.0    0.0    0.0 -0.108492 -0.166980   \n",
       "2     ...    0.0    0.0    0.0    0.0    0.0    0.0  0.620128 -0.166980   \n",
       "3     ...    0.0    0.0    0.0    0.0    0.0    0.0 -0.260994 -0.166980   \n",
       "4     ...    0.0    0.0    0.0    0.0    0.0    0.0  0.416792 -0.166980   \n",
       "...   ...    ...    ...    ...    ...    ...    ...       ...       ...   \n",
       "3995  ...    0.0    0.0    0.0    0.0    0.0    0.0 -0.193216  0.014234   \n",
       "3996  ...    0.0    0.0    0.0    0.0    0.0    0.0 -0.260994 -0.179061   \n",
       "3997  ...    0.0    0.0    0.0    0.0    0.0    0.0  0.145678 -0.034089   \n",
       "3998  ...    0.0    0.0    0.0    0.0    0.0    0.0 -0.260994 -0.166980   \n",
       "3999  ...    0.0    0.0    0.0    0.0    0.0    0.0 -0.193216  0.014234   \n",
       "\n",
       "         2         3      \n",
       "0    -1.394170  0.014839  \n",
       "1    -1.394170 -0.797496  \n",
       "2    -1.394170 -0.425001  \n",
       "3    -1.395813  0.407228  \n",
       "4    -1.395813  2.607957  \n",
       "...        ...       ...  \n",
       "3995  2.343613  1.693465  \n",
       "3996  2.341970 -1.028203  \n",
       "3997  2.341970  0.382072  \n",
       "3998  2.341970  0.949919  \n",
       "3999  2.341970  1.361815  \n",
       "\n",
       "[4000 rows x 20004 columns]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating column names from features\n",
    "\n",
    "features = tfidf.get_feature_names() + ['num_comments', 'score', 'days_old', 'log_all_text_length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['log_all_text_length',\n",
       " 'days_old',\n",
       " 'score',\n",
       " 'num_comments',\n",
       " '',\n",
       " '',\n",
       " 'zy 2jihqeg4gwhnbieym',\n",
       " 'zy',\n",
       " 'zuckerberg',\n",
       " 'zpgcrsg5de1xali3ftuh3fcnculuzqbzn0l5incqqmu',\n",
       " 'zone',\n",
       " 'zionist',\n",
       " 'zionism',\n",
       " 'ziegler',\n",
       " 'zero',\n",
       " 'zellerbach hall',\n",
       " 'zellerbach',\n",
       " 'zelenskiy',\n",
       " 'zeid',\n",
       " 'zealand',\n",
       " 'zakaria',\n",
       " 'youtubers',\n",
       " 'youtuber',\n",
       " 'youtube video',\n",
       " 'youtube http',\n",
       " 'youtube com',\n",
       " 'youtube channel',\n",
       " 'youtube',\n",
       " 'youtu',\n",
       " 'youth',\n",
       " 'youre',\n",
       " 'younger people',\n",
       " 'younger generation',\n",
       " 'younger',\n",
       " 'young woman',\n",
       " 'young turk',\n",
       " 'young people',\n",
       " 'young men',\n",
       " 'young man',\n",
       " 'young liberal',\n",
       " 'young conservative',\n",
       " 'young age',\n",
       " 'young',\n",
       " 'yorkers',\n",
       " 'york time',\n",
       " 'york city',\n",
       " 'york 202',\n",
       " 'york',\n",
       " 'yingyangapp yingyang',\n",
       " 'yingyangapp',\n",
       " 'yingyang',\n",
       " 'yield',\n",
       " 'yiannopoulos shut',\n",
       " 'yiannopoulos',\n",
       " 'yesterday',\n",
       " 'yes know',\n",
       " 'yes',\n",
       " 'yep',\n",
       " 'yen',\n",
       " 'yemen',\n",
       " 'yellow',\n",
       " 'yelling',\n",
       " 'yell',\n",
       " 'yee',\n",
       " 'years',\n",
       " 'yearning breathe',\n",
       " 'yearning',\n",
       " 'year year',\n",
       " 'year week',\n",
       " 'year wa',\n",
       " 'year ve',\n",
       " 'year trump',\n",
       " 'year time',\n",
       " 'year thousand',\n",
       " 'year term',\n",
       " 'year state',\n",
       " 'year shall',\n",
       " 'year section',\n",
       " 'year republican',\n",
       " 'year ratification',\n",
       " 'year period',\n",
       " 'year old',\n",
       " 'year obama',\n",
       " 'year million',\n",
       " 'year make',\n",
       " 'year like',\n",
       " 'year life',\n",
       " 'year later',\n",
       " 'year just',\n",
       " 'year http',\n",
       " 'year half',\n",
       " 'year ha',\n",
       " 'year going',\n",
       " 'year federal',\n",
       " 'year end',\n",
       " 'year election',\n",
       " 'year earlier',\n",
       " 'year don',\n",
       " 'year did',\n",
       " 'year date',\n",
       " 'year congress',\n",
       " 'year college',\n",
       " 'year budget',\n",
       " 'year black',\n",
       " 'year ban',\n",
       " 'year american',\n",
       " 'year ago',\n",
       " 'year age',\n",
       " 'year',\n",
       " 'yeah yeah',\n",
       " 'yeah oh',\n",
       " 'yeah',\n",
       " 'yd3ks0jg bk',\n",
       " 'yd3ks0jg',\n",
       " 'yay',\n",
       " 'yates',\n",
       " 'yasmin',\n",
       " 'yarmuth',\n",
       " 'yard',\n",
       " 'yanukovych wa',\n",
       " 'yanukovych',\n",
       " 'yank',\n",
       " 'yang',\n",
       " 'yall',\n",
       " 'yale',\n",
       " 'yahoo com',\n",
       " 'yahoo',\n",
       " 'yada yada',\n",
       " 'yada',\n",
       " 'ya ll',\n",
       " 'ya know',\n",
       " 'ya',\n",
       " 'xyz',\n",
       " 'xviii',\n",
       " 'xpost',\n",
       " 'xl',\n",
       " 'xiaoli',\n",
       " 'xhtml',\n",
       " 'xenophobic',\n",
       " 'xenophobia',\n",
       " 'xenophobes',\n",
       " 'xavier',\n",
       " 'x200b want',\n",
       " 'x200b wa',\n",
       " 'x200b video',\n",
       " 'x200b processing',\n",
       " 'x200b people',\n",
       " 'x200b let',\n",
       " 'x200b img',\n",
       " 'x200b http',\n",
       " 'x200b corporate',\n",
       " 'x200b amp',\n",
       " 'x200b',\n",
       " 'wyomingite',\n",
       " 'wyoming 202',\n",
       " 'wyoming',\n",
       " 'wyden oregon',\n",
       " 'wyden',\n",
       " 'wy sen',\n",
       " 'wy',\n",
       " 'www zerohedge',\n",
       " 'www youtube',\n",
       " 'www wsj',\n",
       " 'www womenasleaders',\n",
       " 'www washingtontimes',\n",
       " 'www washingtonpost',\n",
       " 'www washingtonexaminer',\n",
       " 'www warren',\n",
       " 'www vox',\n",
       " 'www usatoday',\n",
       " 'www thenation',\n",
       " 'www theguardian',\n",
       " 'www thegatewaypundit',\n",
       " 'www thedailybeast',\n",
       " 'www theatlantic',\n",
       " 'www theamericanconservative',\n",
       " 'www telegraph',\n",
       " 'www tedcruz',\n",
       " 'www surveymonkey',\n",
       " 'www state',\n",
       " 'www slate',\n",
       " 'www sec',\n",
       " 'www salon',\n",
       " 'www ropercenter',\n",
       " 'www reuters',\n",
       " 'www redstate',\n",
       " 'www reddit',\n",
       " 'www realclearpolitics',\n",
       " 'www ranker',\n",
       " 'www publicknowledge',\n",
       " 'www politifact',\n",
       " 'www politico',\n",
       " 'www pavoterservices',\n",
       " 'www palmerreport',\n",
       " 'www opensecrets',\n",
       " 'www nytimes',\n",
       " 'www nydailynews',\n",
       " 'www npr',\n",
       " 'www np',\n",
       " 'www nj',\n",
       " 'www newyorker',\n",
       " 'www ncbi',\n",
       " 'www nbcnews',\n",
       " 'www nationalreview',\n",
       " 'www lifesitenews',\n",
       " 'www legis',\n",
       " 'www law',\n",
       " 'www latimes',\n",
       " 'www kickstarter',\n",
       " 'www jpost',\n",
       " 'www instagram',\n",
       " 'www independent',\n",
       " 'www huffingtonpost',\n",
       " 'www heritage',\n",
       " 'www greatbiblestudy',\n",
       " 'www govtrack',\n",
       " 'www gop',\n",
       " 'www google',\n",
       " 'www gallup',\n",
       " 'www fvap',\n",
       " 'www foxnews',\n",
       " 'www forbes',\n",
       " 'www flsenate',\n",
       " 'www facebook',\n",
       " 'www election',\n",
       " 'www eff',\n",
       " 'www donaldjtrump',\n",
       " 'www dailywire',\n",
       " 'www dailymail',\n",
       " 'www dailydot',\n",
       " 'www crowdpac',\n",
       " 'www conventionofstates',\n",
       " 'www cnn',\n",
       " 'www cnet',\n",
       " 'www cnbc',\n",
       " 'www change',\n",
       " 'www cdc',\n",
       " 'www cbsnews',\n",
       " 'www businessinsider',\n",
       " 'www brookings',\n",
       " 'www breitbart',\n",
       " 'www bluerevolutionusa',\n",
       " 'www bloomberg',\n",
       " 'www beekay4maine',\n",
       " 'www bbc',\n",
       " 'www archive',\n",
       " 'www antiwesterncosplayers',\n",
       " 'www amazon',\n",
       " 'www aic',\n",
       " 'www aclu',\n",
       " 'www ab',\n",
       " 'www 270towin',\n",
       " 'www',\n",
       " 'wwii',\n",
       " 'ww2',\n",
       " 'wv sen',\n",
       " 'wv gov',\n",
       " 'wv',\n",
       " 'wuhan',\n",
       " 'wto',\n",
       " 'wtf',\n",
       " 'wsj com',\n",
       " 'wsj',\n",
       " 'wrought',\n",
       " 'wrote',\n",
       " 'wrongly',\n",
       " 'wrongdoing',\n",
       " 'wrong wrong',\n",
       " 'wrong way',\n",
       " 'wrong wa',\n",
       " 'wrong think',\n",
       " 'wrong thing',\n",
       " 'wrong post',\n",
       " 'wrong place',\n",
       " 'wrong person',\n",
       " 'wrong illegal',\n",
       " 'wrong having',\n",
       " 'wrong don',\n",
       " 'wrong conservative',\n",
       " 'wrong',\n",
       " 'written law',\n",
       " 'written',\n",
       " 'writing today',\n",
       " 'writing paper',\n",
       " 'writing',\n",
       " 'writes',\n",
       " 'writer',\n",
       " 'write',\n",
       " 'wretched refuse',\n",
       " 'wretched',\n",
       " 'wren',\n",
       " 'wreck',\n",
       " 'wrap head',\n",
       " 'wrap',\n",
       " 'wprov sfti1',\n",
       " 'wprov',\n",
       " 'wp content',\n",
       " 'wp 2016',\n",
       " 'wp',\n",
       " 'wow',\n",
       " 'wound',\n",
       " 'wouldnt',\n",
       " 'wouldn want',\n",
       " 'wouldn need',\n",
       " 'wouldn matter',\n",
       " 'wouldn make',\n",
       " 'wouldn like',\n",
       " 'wouldn just',\n",
       " 'wouldn help',\n",
       " 'wouldn better',\n",
       " 'wouldn',\n",
       " 'worthy',\n",
       " 'worthwhile',\n",
       " 'worthless',\n",
       " 'worth',\n",
       " 'worst thing',\n",
       " 'worst fear',\n",
       " 'worst case',\n",
       " 'worst',\n",
       " 'worship',\n",
       " 'worsens democrat',\n",
       " 'worsens',\n",
       " 'worse',\n",
       " 'worrying',\n",
       " 'worry',\n",
       " 'worried leftist',\n",
       " 'worried',\n",
       " 'worn',\n",
       " 'worm',\n",
       " 'worldwide',\n",
       " 'worldview',\n",
       " 'worldnews',\n",
       " 'world year',\n",
       " 'world work',\n",
       " 'world war',\n",
       " 'world want',\n",
       " 'world wa',\n",
       " 'world view',\n",
       " 'world trump',\n",
       " 'world trade',\n",
       " 'world today',\n",
       " 'world thing',\n",
       " 'world stage',\n",
       " 'world problem',\n",
       " 'world powerful',\n",
       " 'world population',\n",
       " 'world peace',\n",
       " 'world need',\n",
       " 'world nation',\n",
       " 'world muslim',\n",
       " 'world let',\n",
       " 'world leader',\n",
       " 'world largest',\n",
       " 'world know',\n",
       " 'world just',\n",
       " 'world ha',\n",
       " 'world europe',\n",
       " 'world economy',\n",
       " 'world doe',\n",
       " 'world cup',\n",
       " 'world country',\n",
       " 'world championship',\n",
       " 'world better',\n",
       " 'world athletics',\n",
       " 'world asia',\n",
       " 'world america',\n",
       " 'world',\n",
       " 'workweek',\n",
       " 'workshop',\n",
       " 'workplace',\n",
       " 'working time',\n",
       " 'working poor',\n",
       " 'working people',\n",
       " 'working job',\n",
       " 'working hard',\n",
       " 'working good',\n",
       " 'working diligently',\n",
       " 'working class',\n",
       " 'working american',\n",
       " 'working',\n",
       " 'workforce',\n",
       " 'workers',\n",
       " 'worker protection',\n",
       " 'worker job',\n",
       " 'worker gt',\n",
       " 'worker america',\n",
       " 'worker',\n",
       " 'worked hard',\n",
       " 'worked',\n",
       " 'workable',\n",
       " 'work way',\n",
       " 'work want',\n",
       " 'work wa',\n",
       " 'work think',\n",
       " 'work save',\n",
       " 'work relevant',\n",
       " 'work place',\n",
       " 'work party',\n",
       " 'work new',\n",
       " 'work need',\n",
       " 'work make',\n",
       " 'work like',\n",
       " 'work just',\n",
       " 'work job',\n",
       " 'work http',\n",
       " 'work hard',\n",
       " 'work ha',\n",
       " 'work guy',\n",
       " 'work good',\n",
       " 'work free',\n",
       " 'work force',\n",
       " 'work encourage',\n",
       " 'work don',\n",
       " 'work democrat',\n",
       " 'work deal',\n",
       " 'work day',\n",
       " 'work construction',\n",
       " 'work congress',\n",
       " 'work better',\n",
       " 'work',\n",
       " 'wore',\n",
       " 'wordpress com',\n",
       " 'wordpress',\n",
       " 'wording',\n",
       " 'worded way',\n",
       " 'worded',\n",
       " 'word word',\n",
       " 'word wa',\n",
       " 'word thought',\n",
       " 'word like',\n",
       " 'word let',\n",
       " 'word left',\n",
       " 'word ha',\n",
       " 'word don',\n",
       " 'word conservative',\n",
       " 'word',\n",
       " 'woodward',\n",
       " 'wood',\n",
       " 'woo',\n",
       " 'wont',\n",
       " 'wondering wa',\n",
       " 'wondering thought',\n",
       " 'wondering people',\n",
       " 'wondering liberal',\n",
       " 'wondering ha',\n",
       " 'wondering guy',\n",
       " 'wondering',\n",
       " 'wonderfully',\n",
       " 'wonderful president',\n",
       " 'wonderful',\n",
       " 'wondered',\n",
       " 'wonder don',\n",
       " 'wonder conservative',\n",
       " 'wonder',\n",
       " 'won work',\n",
       " 'won win',\n",
       " 'won vote',\n",
       " 'won stop',\n",
       " 'won popular',\n",
       " 'won point',\n",
       " 'won people',\n",
       " 'won pay',\n",
       " 'won landslide',\n",
       " 'won help',\n",
       " 'won election',\n",
       " 'won able',\n",
       " 'won',\n",
       " 'womenasleaders world',\n",
       " 'womenasleaders',\n",
       " 'women right',\n",
       " 'women',\n",
       " 'womb',\n",
       " 'woman woman',\n",
       " 'woman want',\n",
       " 'woman wa',\n",
       " 'woman right',\n",
       " 'woman really',\n",
       " 'woman player',\n",
       " 'woman people',\n",
       " 'woman men',\n",
       " 'woman man',\n",
       " 'woman make',\n",
       " 'woman le',\n",
       " 'woman just',\n",
       " 'woman ha',\n",
       " 'woman elected',\n",
       " 'woman don',\n",
       " 'woman child',\n",
       " 'woman black',\n",
       " 'woman',\n",
       " 'wolfpac',\n",
       " 'wolf',\n",
       " 'woke',\n",
       " 'woe',\n",
       " 'wmd',\n",
       " 'witty',\n",
       " 'witnessed',\n",
       " 'witness',\n",
       " 'withholding',\n",
       " 'withhold',\n",
       " 'withheld',\n",
       " 'withdrawal',\n",
       " 'withdraw',\n",
       " 'witch hunt',\n",
       " 'witch',\n",
       " 'wit',\n",
       " 'wishing',\n",
       " 'wished',\n",
       " 'wish wa',\n",
       " 'wish people',\n",
       " 'wish',\n",
       " 'wise',\n",
       " 'wisdom',\n",
       " 'wisconsin 202',\n",
       " 'wisconsin',\n",
       " 'wire fraud',\n",
       " 'wire',\n",
       " 'wiped',\n",
       " 'wipe',\n",
       " 'winter',\n",
       " 'winning submission',\n",
       " 'winning',\n",
       " 'winner poll',\n",
       " 'winner loser',\n",
       " 'winner',\n",
       " 'winger',\n",
       " 'winged',\n",
       " 'wing republican',\n",
       " 'wing politics',\n",
       " 'wing political',\n",
       " 'wing party',\n",
       " 'wing medium',\n",
       " 'wing extremist',\n",
       " 'wing cause',\n",
       " 'wing',\n",
       " 'window',\n",
       " 'wind',\n",
       " 'win win',\n",
       " 'win prize',\n",
       " 'win primary',\n",
       " 'win november',\n",
       " 'win nomination',\n",
       " 'win house',\n",
       " 'win gop',\n",
       " 'win election',\n",
       " 'win 2020',\n",
       " 'win',\n",
       " 'wilson',\n",
       " 'willingness',\n",
       " 'willingly',\n",
       " 'willing vote',\n",
       " 'willing help',\n",
       " 'willing',\n",
       " 'willie',\n",
       " 'williamson',\n",
       " 'williams',\n",
       " 'william buckley',\n",
       " 'william',\n",
       " 'willfully',\n",
       " 'wildly',\n",
       " 'wilder',\n",
       " 'wild',\n",
       " 'wikipedia org',\n",
       " 'wikipedia http',\n",
       " 'wikipedia common',\n",
       " 'wikipedia',\n",
       " 'wikimedia org',\n",
       " 'wikimedia',\n",
       " 'wikileaks',\n",
       " 'wiki republican_party_presidential_primaries',\n",
       " 'wiki http',\n",
       " 'wiki depleted_uranium',\n",
       " 'wiki constitutiondiscussiontoc',\n",
       " 'wiki',\n",
       " 'wife',\n",
       " 'wielding',\n",
       " 'width',\n",
       " 'widow',\n",
       " 'widespread',\n",
       " 'wider',\n",
       " 'widen market',\n",
       " 'widen',\n",
       " 'widely',\n",
       " 'wide',\n",
       " 'wicker mississippi',\n",
       " 'wicker',\n",
       " 'wicked',\n",
       " 'wichita',\n",
       " 'wi',\n",
       " 'whore',\n",
       " 'whomever',\n",
       " 'wholly',\n",
       " 'wholeheartedly',\n",
       " 'whitey',\n",
       " 'whiteness',\n",
       " 'whitehouse rhode',\n",
       " 'whitehouse gov',\n",
       " 'whitehouse',\n",
       " 'white woman',\n",
       " 'white supremacy',\n",
       " 'white supremacist',\n",
       " 'white society',\n",
       " 'white slave',\n",
       " 'white privilege',\n",
       " 'white person',\n",
       " 'white people',\n",
       " 'white oppression',\n",
       " 'white nationalist',\n",
       " 'white nationalism',\n",
       " 'white men',\n",
       " 'white man',\n",
       " 'white male',\n",
       " 'white liberal',\n",
       " 'white kid',\n",
       " 'white house',\n",
       " 'white guy',\n",
       " 'white guilt',\n",
       " 'white girl',\n",
       " 'white conservative',\n",
       " 'white christian',\n",
       " 'white black',\n",
       " 'white american',\n",
       " 'white',\n",
       " 'whistleblower complaint',\n",
       " 'whistleblower',\n",
       " 'whistle blower',\n",
       " 'whistle',\n",
       " 'whisper',\n",
       " 'whisenant',\n",
       " 'whip',\n",
       " 'whiny',\n",
       " 'whining',\n",
       " 'whine',\n",
       " 'whim',\n",
       " 'whilst',\n",
       " 'whig',\n",
       " 'whichever',\n",
       " 'wheel',\n",
       " 'whatsoever',\n",
       " 'whats',\n",
       " 'wh',\n",
       " 'western world',\n",
       " 'western value',\n",
       " 'western society',\n",
       " 'western nation',\n",
       " 'western culture',\n",
       " 'western cosplayers',\n",
       " 'western art',\n",
       " 'western',\n",
       " 'west wing',\n",
       " 'west virginia',\n",
       " 'west coast',\n",
       " 'west',\n",
       " 'weren',\n",
       " 'went school',\n",
       " 'went far',\n",
       " 'went',\n",
       " 'wendy davis',\n",
       " 'wendy',\n",
       " 'welfare state',\n",
       " 'welfare program',\n",
       " 'welfare food',\n",
       " 'welfare check',\n",
       " 'welfare benefit',\n",
       " 'welfare',\n",
       " 'weld',\n",
       " 'welcomed',\n",
       " 'welcome just',\n",
       " 'welcome conservative',\n",
       " 'welcome',\n",
       " 'weird',\n",
       " 'weinstein',\n",
       " 'weight',\n",
       " 'weighing',\n",
       " 'weighed',\n",
       " 'weigh',\n",
       " 'weekly',\n",
       " 'weekend',\n",
       " 'week year',\n",
       " 'week wa',\n",
       " 'week trump',\n",
       " 'week sidebar',\n",
       " 'week month',\n",
       " 'week http',\n",
       " 'week conservative',\n",
       " 'week ago',\n",
       " 'week',\n",
       " 'weed',\n",
       " 'wednesday',\n",
       " 'wedding cake',\n",
       " 'wedding',\n",
       " 'website http',\n",
       " 'website',\n",
       " 'webb',\n",
       " 'webapps',\n",
       " 'web2x',\n",
       " 'web design',\n",
       " 'web',\n",
       " 'weather',\n",
       " 'wearing',\n",
       " 'wear',\n",
       " 'weapon war',\n",
       " 'weapon ban',\n",
       " 'weapon',\n",
       " 'wealthy',\n",
       " 'wealthier',\n",
       " 'wealth nation',\n",
       " 'wealth inequality',\n",
       " 'wealth',\n",
       " 'weakness',\n",
       " 'weaker',\n",
       " 'weaken',\n",
       " 'weak',\n",
       " 'way want',\n",
       " 'way ve',\n",
       " 'way trump',\n",
       " 'way thinking',\n",
       " 'way think',\n",
       " 'way thing',\n",
       " 'way street',\n",
       " 'way stop',\n",
       " 'way republican',\n",
       " 'way possible',\n",
       " 'way obama',\n",
       " 'way mean',\n",
       " 'way make',\n",
       " 'way life',\n",
       " 'way know',\n",
       " 'way just',\n",
       " 'way going',\n",
       " 'way fight',\n",
       " 'way don',\n",
       " 'way did',\n",
       " 'way country',\n",
       " 'way better',\n",
       " 'way',\n",
       " 'wax',\n",
       " 'wave',\n",
       " 'watt',\n",
       " 'watson',\n",
       " 'watergate',\n",
       " 'water',\n",
       " 'watchlist',\n",
       " 'watching video',\n",
       " 'watching fox',\n",
       " 'watching',\n",
       " 'watched',\n",
       " 'watch youtube',\n",
       " 'watch video',\n",
       " 'watch news',\n",
       " 'watch live',\n",
       " 'watch list',\n",
       " 'watch fox',\n",
       " 'watch cnn',\n",
       " 'watch',\n",
       " 'wasting time',\n",
       " 'wasting',\n",
       " 'wasteney',\n",
       " 'wasted',\n",
       " 'waste time',\n",
       " 'waste',\n",
       " 'wasserman schultz',\n",
       " 'wasserman',\n",
       " 'wasn',\n",
       " 'washingtontimes com',\n",
       " 'washingtontimes',\n",
       " 'washingtonpost com',\n",
       " 'washingtonpost',\n",
       " 'washingtonexaminer com',\n",
       " 'washingtonexaminer',\n",
       " 'washington state',\n",
       " 'washington post',\n",
       " 'washington http',\n",
       " 'washington dc',\n",
       " 'washington 202',\n",
       " 'washington',\n",
       " 'washing',\n",
       " 'washed',\n",
       " 'wash',\n",
       " 'wasabisys com',\n",
       " 'wasabisys',\n",
       " 'wary',\n",
       " 'warrior',\n",
       " 'warren',\n",
       " 'warrantless',\n",
       " 'warranted',\n",
       " 'warrant',\n",
       " 'warns',\n",
       " 'warning',\n",
       " 'warner',\n",
       " 'warned',\n",
       " 'warming hoax',\n",
       " 'warming',\n",
       " 'warm',\n",
       " 'warhead',\n",
       " 'warfare',\n",
       " 'warehouse',\n",
       " 'war zone',\n",
       " 'war word',\n",
       " 'war war',\n",
       " 'war wa',\n",
       " 'war syria',\n",
       " 'war russia',\n",
       " 'war president',\n",
       " 'war obama',\n",
       " 'war islam',\n",
       " 'war ii',\n",
       " 'war http',\n",
       " 'war drug',\n",
       " 'war',\n",
       " 'wapo',\n",
       " 'wanting',\n",
       " 'wanted thought',\n",
       " 'wanted tell',\n",
       " 'wanted share',\n",
       " 'wanted say',\n",
       " 'wanted know',\n",
       " 'wanted hear',\n",
       " 'wanted ask',\n",
       " 'wanted',\n",
       " 'want work',\n",
       " 'want witness',\n",
       " 'want use',\n",
       " 'want understand',\n",
       " 'want try',\n",
       " 'want trump',\n",
       " 'want thing',\n",
       " 'want thank',\n",
       " 'want tax',\n",
       " 'want talk',\n",
       " 'want stop',\n",
       " 'want stay',\n",
       " 'want start',\n",
       " 'want say',\n",
       " 'want president',\n",
       " 'want people',\n",
       " 'want pay',\n",
       " 'want opinion',\n",
       " 'want open',\n",
       " 'want make',\n",
       " 'want live',\n",
       " 'want listen',\n",
       " 'want liberal',\n",
       " 'want let',\n",
       " 'want leave',\n",
       " 'want learn',\n",
       " 'want know',\n",
       " 'want kill',\n",
       " 'want just',\n",
       " 'want join',\n",
       " 'want help',\n",
       " 'want hear',\n",
       " 'want government',\n",
       " 'want feel',\n",
       " 'want fact',\n",
       " 'want end',\n",
       " 'want don',\n",
       " 'want destroy',\n",
       " 'want create',\n",
       " 'want come',\n",
       " 'want change',\n",
       " 'want best',\n",
       " 'want believe',\n",
       " 'want ban',\n",
       " 'want away',\n",
       " 'want ask',\n",
       " 'want argue',\n",
       " 'want apologize',\n",
       " 'want america',\n",
       " 'want',\n",
       " 'wanna know',\n",
       " 'wanna',\n",
       " 'walter',\n",
       " 'walsh',\n",
       " 'walmart',\n",
       " 'wallet',\n",
       " 'wallace',\n",
       " 'wall street',\n",
       " 'wall',\n",
       " 'walking',\n",
       " 'walker',\n",
       " 'walked away',\n",
       " 'walked',\n",
       " 'walkaway movement',\n",
       " 'walkaway',\n",
       " 'walk away',\n",
       " 'walk',\n",
       " 'wake',\n",
       " 'waiting',\n",
       " 'waited',\n",
       " 'wait',\n",
       " 'wage gap',\n",
       " 'wage',\n",
       " 'wade',\n",
       " 'wa younger',\n",
       " 'wa young',\n",
       " 'wa year',\n",
       " 'wa wrong',\n",
       " 'wa written',\n",
       " 'wa wondering',\n",
       " 'wa white',\n",
       " 'wa way',\n",
       " 'wa watching',\n",
       " 'wa walking',\n",
       " 'wa wa',\n",
       " 'wa video',\n",
       " 'wa used',\n",
       " 'wa unconstitutional',\n",
       " 'wa trying',\n",
       " 'wa trump',\n",
       " 'wa truly',\n",
       " 'wa true',\n",
       " 'wa told',\n",
       " 'wa time',\n",
       " 'wa thought',\n",
       " 'wa thinking',\n",
       " 'wa terrible',\n",
       " 'wa talking',\n",
       " 'wa taken',\n",
       " 'wa surprised',\n",
       " 'wa smart',\n",
       " 'wa shot',\n",
       " 'wa shocked',\n",
       " 'wa selected',\n",
       " 'wa saying',\n",
       " 'wa running',\n",
       " 'wa room',\n",
       " 'wa right',\n",
       " 'wa revealed',\n",
       " 'wa result',\n",
       " 'wa republican',\n",
       " 'wa removed',\n",
       " 'wa released',\n",
       " 'wa recently',\n",
       " 'wa reason',\n",
       " 'wa really',\n",
       " 'wa real',\n",
       " 'wa reading',\n",
       " 'wa raped',\n",
       " 'wa raised',\n",
       " 'wa probably',\n",
       " 'wa pretty',\n",
       " 'wa president',\n",
       " 'wa power',\n",
       " 'wa place',\n",
       " 'wa pissed',\n",
       " 'wa people',\n",
       " 'wa passed',\n",
       " 'wa originally',\n",
       " 'wa office',\n",
       " 'wa necessary',\n",
       " 'wa named',\n",
       " 'wa muslim',\n",
       " 'wa murdered',\n",
       " 'wa mistake',\n",
       " 'wa military',\n",
       " 'wa meant',\n",
       " 'wa man',\n",
       " 'wa major',\n",
       " 'wa lying',\n",
       " 'wa looking',\n",
       " 'wa long',\n",
       " 'wa literally',\n",
       " 'wa like',\n",
       " 'wa liberal',\n",
       " 'wa left',\n",
       " 'wa le',\n",
       " 'wa labeled',\n",
       " 'wa just',\n",
       " 'wa involved',\n",
       " 'wa intended',\n",
       " 'wa important',\n",
       " 'wa impeached',\n",
       " 'wa hoping',\n",
       " 'wa having',\n",
       " 'wa happy',\n",
       " 'wa happening',\n",
       " 'wa greatest',\n",
       " 'wa great',\n",
       " 'wa government',\n",
       " 'wa good',\n",
       " 'wa going',\n",
       " 'wa giving',\n",
       " 'wa given',\n",
       " 'wa founded',\n",
       " 'wa forced',\n",
       " 'wa fired',\n",
       " 'wa fascinating',\n",
       " 'wa far',\n",
       " 'wa extremely',\n",
       " 'wa eventually',\n",
       " 'wa established',\n",
       " 'wa elected',\n",
       " 'wa don',\n",
       " 'wa doing',\n",
       " 'wa dead',\n",
       " 'wa curious',\n",
       " 'wa created',\n",
       " 'wa convicted',\n",
       " 'wa conservative',\n",
       " ...]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_combined.columns = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>00 hour</th>\n",
       "      <th>00 joe</th>\n",
       "      <th>00 jose</th>\n",
       "      <th>00 month</th>\n",
       "      <th>00 year</th>\n",
       "      <th>000</th>\n",
       "      <th>000 00</th>\n",
       "      <th>000 000</th>\n",
       "      <th>000 death</th>\n",
       "      <th>...</th>\n",
       "      <th>zpgcrsg5de1xali3ftuh3fcnculuzqbzn0l5incqqmu</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>zy</th>\n",
       "      <th>zy 2jihqeg4gwhnbieym</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>days_old</th>\n",
       "      <th>log_all_text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.023769</td>\n",
       "      <td>-0.166980</td>\n",
       "      <td>-1.394170</td>\n",
       "      <td>0.014839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.108492</td>\n",
       "      <td>-0.166980</td>\n",
       "      <td>-1.394170</td>\n",
       "      <td>-0.797496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.620128</td>\n",
       "      <td>-0.166980</td>\n",
       "      <td>-1.394170</td>\n",
       "      <td>-0.425001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.260994</td>\n",
       "      <td>-0.166980</td>\n",
       "      <td>-1.395813</td>\n",
       "      <td>0.407228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.036584</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.416792</td>\n",
       "      <td>-0.166980</td>\n",
       "      <td>-1.395813</td>\n",
       "      <td>2.607957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3995</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.193216</td>\n",
       "      <td>0.014234</td>\n",
       "      <td>2.343613</td>\n",
       "      <td>1.693465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.260994</td>\n",
       "      <td>-0.179061</td>\n",
       "      <td>2.341970</td>\n",
       "      <td>-1.028203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.145678</td>\n",
       "      <td>-0.034089</td>\n",
       "      <td>2.341970</td>\n",
       "      <td>0.382072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.162015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.260994</td>\n",
       "      <td>-0.166980</td>\n",
       "      <td>2.341970</td>\n",
       "      <td>0.949919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.193216</td>\n",
       "      <td>0.014234</td>\n",
       "      <td>2.341970</td>\n",
       "      <td>1.361815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4000 rows  20004 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       00  00 hour  00 joe  00 jose  00 month  00 year       000  000 00  \\\n",
       "0     0.0      0.0     0.0      0.0       0.0      0.0  0.000000     0.0   \n",
       "1     0.0      0.0     0.0      0.0       0.0      0.0  0.000000     0.0   \n",
       "2     0.0      0.0     0.0      0.0       0.0      0.0  0.000000     0.0   \n",
       "3     0.0      0.0     0.0      0.0       0.0      0.0  0.000000     0.0   \n",
       "4     0.0      0.0     0.0      0.0       0.0      0.0  0.036584     0.0   \n",
       "...   ...      ...     ...      ...       ...      ...       ...     ...   \n",
       "3995  0.0      0.0     0.0      0.0       0.0      0.0  0.000000     0.0   \n",
       "3996  0.0      0.0     0.0      0.0       0.0      0.0  0.000000     0.0   \n",
       "3997  0.0      0.0     0.0      0.0       0.0      0.0  0.000000     0.0   \n",
       "3998  0.0      0.0     0.0      0.0       0.0      0.0  0.162015     0.0   \n",
       "3999  0.0      0.0     0.0      0.0       0.0      0.0  0.000000     0.0   \n",
       "\n",
       "      000 000  000 death  ...  zpgcrsg5de1xali3ftuh3fcnculuzqbzn0l5incqqmu  \\\n",
       "0         0.0        0.0  ...                                          0.0   \n",
       "1         0.0        0.0  ...                                          0.0   \n",
       "2         0.0        0.0  ...                                          0.0   \n",
       "3         0.0        0.0  ...                                          0.0   \n",
       "4         0.0        0.0  ...                                          0.0   \n",
       "...       ...        ...  ...                                          ...   \n",
       "3995      0.0        0.0  ...                                          0.0   \n",
       "3996      0.0        0.0  ...                                          0.0   \n",
       "3997      0.0        0.0  ...                                          0.0   \n",
       "3998      0.0        0.0  ...                                          0.0   \n",
       "3999      0.0        0.0  ...                                          0.0   \n",
       "\n",
       "      zuckerberg   zy  zy 2jihqeg4gwhnbieym       num_comments     score  \\\n",
       "0            0.0  0.0                   0.0  0.0  0.0     -0.023769 -0.166980   \n",
       "1            0.0  0.0                   0.0  0.0  0.0     -0.108492 -0.166980   \n",
       "2            0.0  0.0                   0.0  0.0  0.0      0.620128 -0.166980   \n",
       "3            0.0  0.0                   0.0  0.0  0.0     -0.260994 -0.166980   \n",
       "4            0.0  0.0                   0.0  0.0  0.0      0.416792 -0.166980   \n",
       "...          ...  ...                   ...  ...  ...           ...       ...   \n",
       "3995         0.0  0.0                   0.0  0.0  0.0     -0.193216  0.014234   \n",
       "3996         0.0  0.0                   0.0  0.0  0.0     -0.260994 -0.179061   \n",
       "3997         0.0  0.0                   0.0  0.0  0.0      0.145678 -0.034089   \n",
       "3998         0.0  0.0                   0.0  0.0  0.0     -0.260994 -0.166980   \n",
       "3999         0.0  0.0                   0.0  0.0  0.0     -0.193216  0.014234   \n",
       "\n",
       "      days_old  log_all_text_length  \n",
       "0    -1.394170             0.014839  \n",
       "1    -1.394170            -0.797496  \n",
       "2    -1.394170            -0.425001  \n",
       "3    -1.395813             0.407228  \n",
       "4    -1.395813             2.607957  \n",
       "...        ...                  ...  \n",
       "3995  2.343613             1.693465  \n",
       "3996  2.341970            -1.028203  \n",
       "3997  2.341970             0.382072  \n",
       "3998  2.341970             0.949919  \n",
       "3999  2.341970             1.361815  \n",
       "\n",
       "[4000 rows x 20004 columns]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split on combined data\n",
    "\n",
    "X_combined_train, X_combined_test, y_combined_train, y_combined_test = train_test_split(X_combined, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiating model, fitting, and scoring\n",
    "\n",
    "svc_custom = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_custom.fit(X_combined_train, y_combined_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score_svc = svc_custom.score(X_combined_train, y_combined_train)\n",
    "test_score_svc = svc_custom.score(X_combined_test, y_combined_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = svc_custom.predict(X_combined_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_combined_test,\n",
    "                                  preds).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "specificity = round(tn / (tn + fp), 4)\n",
    "sensitivity = round(tp / (tp + fn), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Accuracy: 0.5\n",
      "Train Accuracy: 0.827\n",
      "Test Accuracy: 0.783\n",
      "\n",
      "True Positives: 433\n",
      "True Negatives: 350\n",
      "False Positives: 163\n",
      "False Negatives: 54\n",
      "\n",
      "Specificity: 0.6823\n",
      "Sensitivity: 0.8891\n"
     ]
    }
   ],
   "source": [
    "print(\"Baseline Accuracy: 0.5\")\n",
    "print(f\"Train Accuracy: {round(train_score_svc, 4)}\")\n",
    "print(f\"Test Accuracy: {round(test_score_svc, 4)}\")\n",
    "print()\n",
    "print(f\"True Positives: {tp}\")\n",
    "print(f\"True Negatives: {tn}\")\n",
    "print(f\"False Positives: {fp}\")\n",
    "print(f\"False Negatives: {fn}\")\n",
    "print()\n",
    "print(f\"Specificity: {specificity}\")\n",
    "print(f\"Sensitivity: {sensitivity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer/BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting parameters, creating pipeline, and scoring model\n",
    "\n",
    "params_cvec_ss_nb = {\n",
    "    'cvec__ngram_range' : [(1, 1), (1, 2)],\n",
    "    'cvec__lowercase' : [ False],\n",
    "    'cvec__stop_words' : ['english'],\n",
    "    'ss__with_mean' : [False],\n",
    "    'nb__alpha' : [0.001, 0.01]\n",
    "}\n",
    "\n",
    "pipe_cvec_ss_nb = Pipeline([\n",
    "    ('cvec', CountVectorizer()),\n",
    "    ('ss', StandardScaler()),\n",
    "    ('nb', BernoulliNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------Title---------------------\n",
      "Baseline Accuracy: 0.5\n",
      "Train Accuracy: 0.9823\n",
      "Test Accuracy: 0.619\n",
      "\n",
      "True Positives: 368\n",
      "True Negatives: 251\n",
      "False Positives: 237\n",
      "False Negatives: 144\n",
      "\n",
      "Specificity: 0.5143\n",
      "Sensitivity: 0.7188\n",
      "\n",
      "{'cvec__lowercase': False, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': 'english', 'nb__alpha': 0.001, 'ss__with_mean': False}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------------Self Text---------------------\n",
      "Baseline Accuracy: 0.5\n",
      "Train Accuracy: 0.9667\n",
      "Test Accuracy: 0.594\n",
      "\n",
      "True Positives: 436\n",
      "True Negatives: 158\n",
      "False Positives: 342\n",
      "False Negatives: 64\n",
      "\n",
      "Specificity: 0.316\n",
      "Sensitivity: 0.872\n",
      "\n",
      "{'cvec__lowercase': False, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': 'english', 'nb__alpha': 0.001, 'ss__with_mean': False}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------------All Text---------------------\n",
      "Baseline Accuracy: 0.5\n",
      "Train Accuracy: 0.9667\n",
      "Test Accuracy: 0.594\n",
      "\n",
      "True Positives: 436\n",
      "True Negatives: 158\n",
      "False Positives: 342\n",
      "False Negatives: 64\n",
      "\n",
      "Specificity: 0.316\n",
      "Sensitivity: 0.872\n",
      "\n",
      "{'cvec__lowercase': False, 'cvec__ngram_range': (1, 2), 'cvec__stop_words': 'english', 'nb__alpha': 0.001, 'ss__with_mean': False}\n"
     ]
    }
   ],
   "source": [
    "grid_scores_difftexts(pipe_model = pipe_cvec_ss_nb, \n",
    "                 params = params_cvec_ss_nb, \n",
    "                 cv = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## TfidfVectorizer/BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting parameters, creating pipeline, and scoring model\n",
    "\n",
    "params_tfidf_nb = {\n",
    "    'tfidf__ngram_range' : [(1, 1), (1, 2)],\n",
    "    'tfidf__lowercase' : [ False],\n",
    "    'tfidf__stop_words' : ['english'],\n",
    "    'nb__alpha' : [0.01, 1, 10]\n",
    "}\n",
    "\n",
    "pipe_tfidf_nb = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('nb', BernoulliNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------Title---------------------\n",
      "Baseline Accuracy: 0.5\n",
      "Train Accuracy: 0.7743\n",
      "Test Accuracy: 0.618\n",
      "\n",
      "True Positives: 389\n",
      "True Negatives: 229\n",
      "False Positives: 259\n",
      "False Negatives: 123\n",
      "\n",
      "Specificity: 0.4693\n",
      "Sensitivity: 0.7598\n",
      "\n",
      "{'nb__alpha': 10, 'tfidf__lowercase': False, 'tfidf__ngram_range': (1, 1), 'tfidf__stop_words': 'english'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------------Self Text---------------------\n",
      "Baseline Accuracy: 0.5\n",
      "Train Accuracy: 0.7897\n",
      "Test Accuracy: 0.585\n",
      "\n",
      "True Positives: 393\n",
      "True Negatives: 192\n",
      "False Positives: 308\n",
      "False Negatives: 107\n",
      "\n",
      "Specificity: 0.384\n",
      "Sensitivity: 0.786\n",
      "\n",
      "{'nb__alpha': 0.01, 'tfidf__lowercase': False, 'tfidf__ngram_range': (1, 1), 'tfidf__stop_words': 'english'}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "---------------------All Text---------------------\n",
      "Baseline Accuracy: 0.5\n",
      "Train Accuracy: 0.7897\n",
      "Test Accuracy: 0.585\n",
      "\n",
      "True Positives: 393\n",
      "True Negatives: 192\n",
      "False Positives: 308\n",
      "False Negatives: 107\n",
      "\n",
      "Specificity: 0.384\n",
      "Sensitivity: 0.786\n",
      "\n",
      "{'nb__alpha': 0.01, 'tfidf__lowercase': False, 'tfidf__ngram_range': (1, 1), 'tfidf__stop_words': 'english'}\n"
     ]
    }
   ],
   "source": [
    "grid_scores_difftexts(pipe_model = pipe_tfidf_nb, \n",
    "                 params = params_tfidf_nb, \n",
    "                 cv = 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
